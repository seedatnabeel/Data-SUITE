{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3de74733",
   "metadata": {
    "_uuid": "a171a930e23459ecbdb4d50dbffd2c18ca8e8ced"
   },
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106d72c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(\"..\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1720c029",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from aix360.algorithms.protodash import ProtodashExplainer\n",
    "from src.utils.data_utils import read_from_file\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(\"..\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "artifact_path = \"artifacts\"\n",
    "results_path = \"results\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37d8f4a",
   "metadata": {
    "_uuid": "fbbeca12041ad15f3af35a5554028ee2dfa4c105",
    "jupyter": {
     "outputs_hidden": true
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# set plot parameters\n",
    "SMALL_SIZE = 14\n",
    "MEDIUM_SIZE = 16\n",
    "BIGGER_SIZE = 20\n",
    "\n",
    "plt.rc(\"font\", size=SMALL_SIZE)  # controls default text sizes\n",
    "plt.rc(\"axes\", titlesize=SMALL_SIZE)  # fontsize of the axes title\n",
    "plt.rc(\"axes\", labelsize=12)  # fontsize of the x and y labels\n",
    "plt.rc(\"xtick\", labelsize=12)  # fontsize of the tick labels\n",
    "plt.rc(\"ytick\", labelsize=12)  # fontsize of the tick labels\n",
    "plt.rc(\"legend\", fontsize=SMALL_SIZE)  # legend fontsize\n",
    "plt.rc(\"figure\", titlesize=BIGGER_SIZE)  # fontsize of the figure title\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00680d9",
   "metadata": {},
   "source": [
    "### Load up artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52c836c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "artifacts = read_from_file(f\"{artifact_path}/adult_dataset_artifacts.p\")\n",
    "model_ids = read_from_file(f\"{artifact_path}/adult_model_ids.p\")\n",
    "\n",
    "\n",
    "# unpack from artifacts\n",
    "clf = artifacts[\"clf\"]\n",
    "\n",
    "conformal_dict = artifacts[\"conformal_dict\"]\n",
    "inliers_dict = artifacts[\"inliers_dict\"]\n",
    "outliers_dict = artifacts[\"outliers_dict\"]\n",
    "benchmark_mean = artifacts[\"benchmark_mean\"]\n",
    "benchmark_std = artifacts[\"benchmark_std\"]\n",
    "small_ci_ids = artifacts[\"small_ci_ids\"]\n",
    "large_ci_ids = artifacts[\"large_ci_ids\"]\n",
    "df_out = artifacts[\"df_out\"]\n",
    "X_train = artifacts[\"X_train\"]\n",
    "y_train = artifacts[\"y_train\"]\n",
    "X_test = artifacts[\"X_test\"]\n",
    "y_test = artifacts[\"y_test\"]\n",
    "X = artifacts[\"X\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207daa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e729e03e",
   "metadata": {},
   "source": [
    "# MPI RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e23cc88",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "MPI_df = pd.DataFrame([benchmark_mean, benchmark_std], index=[\"Mean\", \"Std\"])\n",
    "\n",
    "MPI_df.to_csv(f\"{results_path}/adult/MPI_df.csv\")\n",
    "\n",
    "MPI_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f2c01d",
   "metadata": {},
   "source": [
    "## Group feature comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa4e039",
   "metadata": {},
   "source": [
    "### Run Protodash as a comparison & assess correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602b9002",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = ProtodashExplainer()\n",
    "\n",
    "m = 100\n",
    "(W, S, setValues) = explainer.explain(\n",
    "    X_test, X_train, m=m, kernelType=\"Gaussian\", sigma=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293e71bb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "cert_ids = small_ci_ids[0:100]\n",
    "uncert_ids = large_ci_ids[-100:]\n",
    "\n",
    "vals = []\n",
    "for i in range(X_train.shape[1]):\n",
    "    vals.append(\n",
    "        stats.spearmanr(np.sort(X_train[S, i]), np.sort(X_test[cert_ids, i]))[0]\n",
    "    )\n",
    "\n",
    "print(f\"Correlation between prototypes vs certain: {np.nanmean(vals)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef676a25",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "vals = []\n",
    "for i in range(X_train.shape[1]):\n",
    "    vals.append(\n",
    "        np.corrcoef(np.sort(X_train[S, i]), np.sort(X_test[uncert_ids, i]))[0, 1]\n",
    "    )\n",
    "\n",
    "print(f\"Correlation between prototypes vs uncertain: {np.nanmean(vals)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc7920c",
   "metadata": {},
   "source": [
    "### Compare the features of the various groups - data insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1133c2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "cert_ids = small_ci_ids[0:100]\n",
    "uncert_ids = large_ci_ids[-100:]\n",
    "\n",
    "uncert_mean = np.mean(X_test[uncert_ids, :], axis=0)\n",
    "cert_mean = np.mean(X_test[cert_ids, :], axis=0)\n",
    "train_mean = np.mean(X_train, axis=0)\n",
    "test_mean = np.mean(X_test, axis=0)\n",
    "cert_mean = np.mean(X_test[cert_ids, :], axis=0)\n",
    "\n",
    "proto_mean = np.mean(X_train[S, :], axis=0)\n",
    "\n",
    "\n",
    "# comparison_df = pd.DataFrame({'proto': proto_mean, 'certain': cert_mean,'uncertain':uncert_mean, \"train\": train_mean, \"test\": test_mean}, index=X.columns)\n",
    "comparison_df = pd.DataFrame(\n",
    "    {\n",
    "        \"certain\": cert_mean,\n",
    "        \"uncertain\": uncert_mean,\n",
    "        \"protodash\": proto_mean,\n",
    "        \"train\": train_mean,\n",
    "        \"test\": test_mean,\n",
    "    },\n",
    "    index=X.columns,\n",
    ")\n",
    "\n",
    "comparison_df.to_csv(f\"{results_path}/adult/comparison_df.csv\")\n",
    "\n",
    "comparison_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763e020a",
   "metadata": {},
   "source": [
    "# Data-insights figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258b5c67",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "train_sc = scaler.transform(X_train)\n",
    "test_sc = scaler.transform(X_test)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pcs_train = pca.fit_transform(train_sc)\n",
    "pcs_test = pca.transform(test_sc)\n",
    "\n",
    "\n",
    "n_ids = 100\n",
    "cert_ids = small_ci_ids[0:n_ids]  # certain[0:100]\n",
    "uncert_ids = large_ci_ids[-n_ids:]\n",
    "\n",
    "n_ids = 100\n",
    "# pca.transform(np.mean(X_test[uncert_ids,:], axis=0).reshape(1, -1))\n",
    "uncert_mean = np.mean(pcs_test[large_ci_ids[-n_ids:], :], axis=0)\n",
    "# pca.transform(np.mean(X_test[cert_ids,:], axis=0).reshape(1, -1))\n",
    "cert_mean = np.mean(pcs_test[small_ci_ids[0:n_ids], :], axis=0)\n",
    "# proto_mean = np.mean(pcs_test[S,:],axis=0) #pca.transform(np.mean(X_test[cert_ids,:], axis=0).reshape(1, -1))\n",
    "\n",
    "\n",
    "plt.scatter(pcs_train[:, 0], pcs_train[:, 1], color=\"0.75\", label=\"Train\")\n",
    "plt.scatter(\n",
    "    pcs_test[cert_ids, 0], pcs_test[cert_ids, 1], color=\"g\", label=\"Test - Certain\"\n",
    ")\n",
    "plt.scatter(\n",
    "    pcs_test[uncert_ids, 0],\n",
    "    pcs_test[uncert_ids, 1],\n",
    "    color=\"r\",\n",
    "    label=\"Test - Uncertain\",\n",
    ")\n",
    "# plt.scatter(pcs_test[S,0],pcs_test[S,1], color='b', label = 'Test - s')\n",
    "plt.scatter(cert_mean[0], cert_mean[1], color=\"k\", s=500, marker=\"*\")\n",
    "plt.scatter(uncert_mean[0], uncert_mean[1], color=\"k\", s=300, marker=\"X\")\n",
    "\n",
    "# plt.scatter(proto_mean[0],proto_mean[1], color='b', s=300, marker='X')\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.title(\"Samples per group: 100\")\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(f\"{results_path}/adult/adult_insights_diagram.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d775e8c",
   "metadata": {},
   "source": [
    "## Lambda sweep plot (flag examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c039466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sort_dict(outliers_dict):\n",
    "    counts_dict = {}\n",
    "    for key in outliers_dict.keys():\n",
    "        out_array = outliers_dict[key]\n",
    "\n",
    "        for val in out_array:\n",
    "            if val not in counts_dict.keys():\n",
    "                counts_dict[val] = 1\n",
    "            else:\n",
    "                counts_dict[val] += 1\n",
    "\n",
    "    sort_dict = {k: v for k, v in sorted(counts_dict.items(), key=lambda item: item[1])}\n",
    "    return sort_dict\n",
    "\n",
    "\n",
    "def inconsistent_lambda_sweep(outliers_dict, X_test, y_test, clf):\n",
    "\n",
    "    incons_scores = []\n",
    "    total_samples = []\n",
    "\n",
    "    sort_dict = get_sort_dict(outliers_dict)\n",
    "\n",
    "    for ns in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
    "        flagged_incons_ids = []\n",
    "        for key in list(sort_dict.keys()):\n",
    "            if sort_dict[key] > ns:\n",
    "                flagged_incons_ids.append(key)\n",
    "\n",
    "        flagged_incons_ids = np.array(flagged_incons_ids)\n",
    "\n",
    "        flagged_incons_ids\n",
    "\n",
    "        myids = flagged_incons_ids\n",
    "        y_true = y_test\n",
    "        y_pred = clf.predict(X_test[myids, :])\n",
    "        total_samples.append(len(flagged_incons_ids))\n",
    "\n",
    "        incons_scores.append(accuracy_score(y_true[myids], y_pred))\n",
    "\n",
    "    return incons_scores, total_samples, sort_dict\n",
    "\n",
    "\n",
    "incons_scores, total_samples, sort_dict = inconsistent_lambda_sweep(\n",
    "    outliers_dict, X_test, y_test, clf\n",
    ")\n",
    "\n",
    "\n",
    "plt.plot(np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) / (X_test.shape[1]), incons_scores)\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    "plt.xlabel(\"Proportion of features - λ\")\n",
    "\n",
    "\n",
    "axes1 = plt.gca()\n",
    "axes2 = axes1.twiny()\n",
    "\n",
    "axes2.set_xticklabels(np.array(total_samples)[3:])\n",
    "\n",
    "axes2.set_xlabel(\"n samples\")\n",
    "axes1.set_xlabel(\"Proportion of features - λ\")\n",
    "plt.savefig(f\"{results_path}/adult/adult_lambda_sweep.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57da8684",
   "metadata": {},
   "source": [
    "### OOD COMPARISON EXPERIMENT - OVERLAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c475be4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def ood_test(X_train, X_test, sort_dict, large_ci_ids, n_sample_threshold=8):\n",
    "\n",
    "    flagged_incons_ids = []\n",
    "    for key in list(sort_dict.keys()):\n",
    "        if sort_dict[key] > n_sample_threshold:\n",
    "            flagged_incons_ids.append(key)\n",
    "\n",
    "    flagged_incons_ids = np.array(flagged_incons_ids)\n",
    "\n",
    "    results = {}\n",
    "    for samps in [50, 100, 150, 200, 250]:\n",
    "        print(\"SAMPLING: \", samps)\n",
    "        n_ids = samps\n",
    "        uncert_ids = large_ci_ids[-n_ids:]\n",
    "        uncert_ids.shape\n",
    "\n",
    "        from pyod.models.iforest import IForest\n",
    "        from pyod.models.copod import COPOD\n",
    "        from pyod.models.lof import LOF\n",
    "\n",
    "        from pyod.models.suod import SUOD\n",
    "\n",
    "        # # initialized a group of outlier detectors for acceleration\n",
    "        detector_list = [\n",
    "            LOF(n_neighbors=15),\n",
    "            LOF(n_neighbors=20),\n",
    "            LOF(n_neighbors=25),\n",
    "            LOF(n_neighbors=35),\n",
    "            COPOD(),\n",
    "            IForest(n_estimators=100, random_state=42),\n",
    "            IForest(n_estimators=200, random_state=42),\n",
    "        ]\n",
    "\n",
    "        # # decide the number of parallel process, and the combination method\n",
    "        # # then clf can be used as any outlier detection model\n",
    "\n",
    "        clfs = [\n",
    "            COPOD(),\n",
    "            IForest(n_estimators=100, random_state=42),\n",
    "            SUOD(\n",
    "                base_estimators=detector_list,\n",
    "                n_jobs=2,\n",
    "                combination=\"average\",\n",
    "                verbose=False,\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        means = []\n",
    "        stds = []\n",
    "        uncert_ood = []\n",
    "        incons_ood = []\n",
    "        clf_id = 0\n",
    "        for clf_current in clfs:\n",
    "            clf_id += 1\n",
    "            print(clf_id)\n",
    "            clf = clf_current\n",
    "            clf.fit(X_train)\n",
    "            probas = clf.predict_proba(X_test)[:, 1]\n",
    "            means.append(np.mean(probas))\n",
    "            stds.append(np.std(probas))\n",
    "\n",
    "            ood = np.argsort(probas)[-len(uncert_ids) :]\n",
    "            i = 0\n",
    "            for val in ood:\n",
    "                if val in uncert_ids:\n",
    "                    i += 1\n",
    "\n",
    "            uncert_ood.append(i / len(ood))\n",
    "\n",
    "            ood = np.argsort(probas)[-len(flagged_incons_ids) :]\n",
    "            i = 0\n",
    "            for val in ood:\n",
    "                if val in flagged_incons_ids:\n",
    "                    i += 1\n",
    "\n",
    "            incons_ood.append(i / len(ood))\n",
    "\n",
    "        results[samps] = {\n",
    "            \"means\": means,\n",
    "            \"stds\": stds,\n",
    "            \"uncert_ood\": uncert_ood,\n",
    "            \"incons_ood\": incons_ood,\n",
    "        }\n",
    "\n",
    "    from alibi_detect.od.mahalanobis import Mahalanobis\n",
    "\n",
    "    od = Mahalanobis(cat_vars={3: 1, 4: 3, 5: 3, 6: 1, 7: 1, 8: 1, 10: 1, 11: 2})\n",
    "    od.fit(X_train)\n",
    "\n",
    "    scores = od.score(X_test)\n",
    "\n",
    "    mahalanobis_incons = []\n",
    "    mahalanobis_uncert = []\n",
    "    for samps in [50, 100, 150, 200, 250]:\n",
    "        print(\"SAMPLING: \", samps)\n",
    "        n_ids = samps\n",
    "        uncert_ids = large_ci_ids[-n_ids:]\n",
    "        ood = np.argsort(scores)[-len(flagged_incons_ids) :]\n",
    "        i = 0\n",
    "        for val in ood:\n",
    "            if val in flagged_incons_ids:\n",
    "                i += 1\n",
    "        mahalanobis_incons.append(i / len(ood))\n",
    "\n",
    "        ood = np.argsort(scores)[-len(uncert_ids) :]\n",
    "        i = 0\n",
    "        for val in ood:\n",
    "            if val in uncert_ids:\n",
    "                i += 1\n",
    "        mahalanobis_uncert.append(i / len(ood))\n",
    "\n",
    "    return results, mahalanobis_incons, mahalanobis_uncert\n",
    "\n",
    "\n",
    "results, mahalanobis_incons, mahalanobis_uncert = ood_test(\n",
    "    X_train, X_test, sort_dict, large_ci_ids, n_sample_threshold=8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe3099e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_arr = []\n",
    "method1_ood = []\n",
    "method2_ood = []\n",
    "method3_ood = []\n",
    "\n",
    "method1_mean = []\n",
    "method2_mean = []\n",
    "method3_mean = []\n",
    "\n",
    "method1_std = []\n",
    "method2_std = []\n",
    "method3_std = []\n",
    "\n",
    "method1_incons = []\n",
    "method2_incons = []\n",
    "method3_incons = []\n",
    "\n",
    "\n",
    "for samps in [50, 100, 150, 200, 250]:\n",
    "\n",
    "    sample_arr.append(samps)\n",
    "\n",
    "    method1_ood.append(results[samps][\"uncert_ood\"][0])\n",
    "    method2_ood.append(results[samps][\"uncert_ood\"][1])\n",
    "    method3_ood.append(results[samps][\"uncert_ood\"][2])\n",
    "\n",
    "    method1_incons.append(results[samps][\"incons_ood\"][0])\n",
    "    method2_incons.append(results[samps][\"incons_ood\"][1])\n",
    "    method3_incons.append(results[samps][\"incons_ood\"][2])\n",
    "\n",
    "    method1_mean.append(results[samps][\"means\"][0])\n",
    "    method2_mean.append(results[samps][\"means\"][1])\n",
    "    method3_mean.append(results[samps][\"means\"][2])\n",
    "\n",
    "    method1_std.append(results[samps][\"stds\"][0])\n",
    "    method2_std.append(results[samps][\"stds\"][1])\n",
    "    method3_std.append(results[samps][\"stds\"][2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3835e0d",
   "metadata": {},
   "source": [
    "### OOD COMPARISON PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c011d072",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "plt.plot(sample_arr[:-1], method1_ood[:-1], marker=\"o\", label=\"COPOD\")\n",
    "plt.plot(sample_arr[:-1], method2_ood[:-1], marker=\"o\", label=\"IForest\")\n",
    "plt.plot(sample_arr[:-1], method3_ood[:-1], marker=\"o\", label=\"SUOD\")\n",
    "plt.plot(sample_arr[:-1], mahalanobis_uncert[:-1], marker=\"o\", label=\"Mahalanobis\")\n",
    "plt.xlabel(\"Top n uncertain samples\")\n",
    "plt.ylabel(\"OOD-Uncertain Sample Match Rate\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"{results_path}/adult/adult_ood_uncertain_match.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141d0da4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "plt.plot(sample_arr, method1_mean, marker=\"o\", label=\"COPOD\")\n",
    "plt.plot(sample_arr, method2_mean, marker=\"o\", label=\"IForest\")\n",
    "plt.plot(sample_arr, method3_mean, marker=\"o\", label=\"SUOD\")\n",
    "plt.xlabel(\"Top n uncertain samples\")\n",
    "plt.ylabel(\"Mean Probability of potential OODs\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"{results_path}/adult/adult_ood_proba.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c919efd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sample_arr[:], method1_incons[:], marker=\"o\", label=\"COPOD\")\n",
    "plt.plot(sample_arr[:], method2_incons[:], marker=\"o\", label=\"IForest\")\n",
    "plt.plot(sample_arr[:], method3_incons[:], marker=\"o\", label=\"SUOD\")\n",
    "plt.plot(sample_arr[:], mahalanobis_incons[:], marker=\"o\", label=\"Mahalanobis\")\n",
    "plt.xlabel(\"Top n inconsistent samples\")\n",
    "plt.ylabel(\"OOD-Inconsistent Sample Match Rate\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"{results_path}/adult/adult_incons_uncertain_match.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f342a7b0",
   "metadata": {},
   "source": [
    "# Diverse downstream model experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd79f0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from aix360.algorithms.protodash import ProtodashExplainer\n",
    "from sklearn_extra.robust import RobustWeightedClassifier\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "model_compare = {}\n",
    "\n",
    "model_list = [\"rf\", \"mlp\", \"gbt\", \"robust\"]\n",
    "\n",
    "\n",
    "for downstream_model in model_list:\n",
    "\n",
    "    logging.info(f\"Testing downstream - {downstream_model}\")\n",
    "    benchmark_mean = {}\n",
    "    benchmark_std = {}\n",
    "    seed = 0\n",
    "\n",
    "    #################################\n",
    "    # Train diverse models\n",
    "    #################################\n",
    "    if downstream_model == \"rf\":\n",
    "        clf = RandomForestClassifier()\n",
    "    if downstream_model == \"mlp\":\n",
    "        clf = MLPClassifier(random_state=seed)\n",
    "    if downstream_model == \"gbt\":\n",
    "        clf = GradientBoostingClassifier()\n",
    "    if downstream_model == \"robust\":\n",
    "        # Scale the dataset with sklearn RobustScaler (important for this algorithm)\n",
    "\n",
    "        clf = RobustWeightedClassifier(\n",
    "            weighting=\"huber\",\n",
    "            loss=\"hinge\",\n",
    "            c=1.35,\n",
    "            eta0=1e-3,\n",
    "            max_iter=300,\n",
    "        )\n",
    "\n",
    "    small_ci_ids = artifacts[\"small_ci_ids\"]\n",
    "    large_ci_ids = artifacts[\"large_ci_ids\"]\n",
    "\n",
    "    if downstream_model != \"robust\":\n",
    "        clf.fit(X_train, y_train)\n",
    "    else:\n",
    "        from copy import deepcopy\n",
    "\n",
    "        Xr = deepcopy(X_train)\n",
    "        sc = RobustScaler()\n",
    "        Xr = sc.fit_transform(Xr)\n",
    "        clf.fit(Xr, y_train)\n",
    "        Xt = deepcopy(X_test)\n",
    "        Xt = sc.transform(Xt)\n",
    "\n",
    "    #################################\n",
    "    # Predict for the different CI levels\n",
    "    #################################\n",
    "    mean_cert = []\n",
    "    for n_ids in range(100, 500, 100):\n",
    "        myids = small_ci_ids[0:n_ids]\n",
    "\n",
    "        y_true = y_test\n",
    "        if downstream_model != \"robust\":\n",
    "            y_pred = clf.predict(X_test[myids, :])\n",
    "        else:\n",
    "            y_pred = clf.predict(Xt[myids, :])\n",
    "\n",
    "        acc_sc = accuracy_score(y_true[myids], y_pred)\n",
    "        mean_cert.append(acc_sc)\n",
    "\n",
    "    mean_cert = np.array(mean_cert)\n",
    "\n",
    "    mean_uncert = []\n",
    "\n",
    "    for n_ids in range(100, 500, 100):\n",
    "        myids = large_ci_ids[-n_ids:]\n",
    "\n",
    "        y_true = y_test\n",
    "        if downstream_model != \"robust\":\n",
    "            y_pred = clf.predict(X_test[myids, :])\n",
    "        else:\n",
    "            y_pred = clf.predict(Xt[myids, :])\n",
    "\n",
    "        acc_sc = accuracy_score(y_true[myids], y_pred)\n",
    "        mean_uncert.append(acc_sc)\n",
    "\n",
    "    mean_uncert = np.array(mean_uncert)\n",
    "\n",
    "    diff_mean = np.mean(mean_cert - mean_uncert)\n",
    "    diff_std = np.std(mean_cert - mean_uncert)\n",
    "\n",
    "    if downstream_model != \"robust\":\n",
    "        benchmark_mean[\"Data-SUITE\"] = diff_mean\n",
    "        benchmark_std[\"Data-SUITE\"] = diff_std\n",
    "    else:\n",
    "        benchmark_mean[\"Data-SUITE\"] = np.abs(diff_mean)\n",
    "        benchmark_std[\"Data-SUITE\"] = np.abs(diff_std)\n",
    "\n",
    "    #################################\n",
    "    # Assess all other baselines\n",
    "    #################################\n",
    "    comparison_models = [\"qr\", \"bnn\", \"conformal\", \"mcd\", \"ensemble\", \"gp\"]\n",
    "\n",
    "    for model in comparison_models:\n",
    "        ordered_scores = model_ids[model]\n",
    "        model_certainty = []\n",
    "        model_uncertainty = []\n",
    "\n",
    "        samples = np.arange(100, 500, 100)\n",
    "        for sample in samples:\n",
    "            certain = ordered_scores[0:sample]\n",
    "            uncertain = ordered_scores[-sample:]\n",
    "\n",
    "            if downstream_model != \"robust\":\n",
    "                y_pred = clf.predict(X_test[certain, :])\n",
    "            else:\n",
    "                y_pred = clf.predict(Xt[certain, :])\n",
    "            model_certainty.append(accuracy_score(y_test[certain], y_pred))\n",
    "\n",
    "            if downstream_model != \"robust\":\n",
    "                y_pred = clf.predict(X_test[uncertain, :])\n",
    "            else:\n",
    "                y_pred = clf.predict(Xt[uncertain, :])\n",
    "\n",
    "            model_uncertainty.append(accuracy_score(y_test[uncertain], y_pred))\n",
    "\n",
    "        diff_mean = np.mean(np.array(model_certainty) - np.array(model_uncertainty))\n",
    "        diff_std = np.std(np.array(model_certainty) - np.array(model_uncertainty))\n",
    "\n",
    "        if downstream_model != \"robust\":\n",
    "            benchmark_mean[model] = diff_mean\n",
    "            benchmark_std[model] = diff_std\n",
    "        else:\n",
    "            benchmark_mean[model] = np.abs(diff_mean)\n",
    "            benchmark_std[model] = np.abs(diff_std)\n",
    "\n",
    "    rank = sorted(benchmark_mean, key=benchmark_mean.get)\n",
    "\n",
    "    model_compare[downstream_model] = rank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4547c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_model_comparison(model_compare):\n",
    "    baselines = [\"ensemble\", \"gp\", \"bnn\", \"conformal\", \"qr\", \"mcd\", \"Data-SUITE\"]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for baseline in baselines:\n",
    "        scores = []\n",
    "        for model in model_list:\n",
    "            index = model_compare[model][::-1].index(baseline) + 1\n",
    "            scores.append(index)\n",
    "        results[baseline] = scores\n",
    "    return results\n",
    "\n",
    "\n",
    "results = process_model_comparison(model_compare)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1cfbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT RESULTS\n",
    "\n",
    "SMALL_SIZE = 10\n",
    "MEDIUM_SIZE = 50\n",
    "BIGGER_SIZE = 50\n",
    "\n",
    "plt.rc(\"font\", size=SMALL_SIZE)  # controls default text sizes\n",
    "plt.rc(\"axes\", titlesize=SMALL_SIZE)  # fontsize of the axes title\n",
    "plt.rc(\"axes\", labelsize=12)  # fontsize of the x and y labels\n",
    "plt.rc(\"xtick\", labelsize=12)  # fontsize of the tick labels\n",
    "plt.rc(\"ytick\", labelsize=12)  # fontsize of the tick labels\n",
    "plt.rc(\"legend\", fontsize=SMALL_SIZE)  # legend fontsize\n",
    "plt.rc(\"figure\", titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.grid()\n",
    "\n",
    "model_list = [mod.upper() for mod in model_list]\n",
    "\n",
    "for key in results.keys():\n",
    "    if key == \"Data-SUITE\":\n",
    "        w = 4\n",
    "    else:\n",
    "        w = 1.5\n",
    "    plt.plot(model_list, results[key], marker=\"o\", label=key.upper(), linewidth=w)\n",
    "\n",
    "plt.gca().invert_yaxis()\n",
    "plt.ylabel(\"MPI rank (HIGHER BETTER)\", fontweight=\"bold\", fontsize=14)\n",
    "plt.xlabel(\"Downstream model type\", fontweight=\"bold\", fontsize=14)\n",
    "plt.xticks(fontsize=15, fontweight=\"bold\")\n",
    "plt.yticks(fontsize=15, fontweight=\"bold\")\n",
    "leg = plt.legend()\n",
    "_, _, _, _, _, _, ds = leg.get_texts()\n",
    "ds.set_size(16)\n",
    "plt.savefig(f\"{results_path}/adult/adult_diverse_model_mpi.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e20e08",
   "metadata": {},
   "source": [
    "# MPI with different downstream models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efd3e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "downstream_model = \"rf\"\n",
    "\n",
    "if downstream_model == \"rf\":\n",
    "    clf = RandomForestClassifier()\n",
    "if downstream_model == \"mlp\":\n",
    "    clf = MLPClassifier()\n",
    "if downstream_model == \"gbt\":\n",
    "    clf = GradientBoostingClassifier()\n",
    "if downstream_model == \"robust\":\n",
    "    # Scale the dataset with sklearn RobustScaler (important for this algorithm)\n",
    "\n",
    "    clf = RobustWeightedClassifier(\n",
    "        weighting=\"mom\",\n",
    "        loss=\"hinge\",\n",
    "        c=1.35,\n",
    "        eta0=1e-3,\n",
    "        max_iter=300,\n",
    "    )\n",
    "if downstream_model == \"robust\":\n",
    "    # Scale the dataset with sklearn RobustScaler (important for this algorithm)\n",
    "\n",
    "    clf = RobustWeightedClassifier(\n",
    "        weighting=\"huber\",\n",
    "        loss=\"hinge\",\n",
    "        c=1.35,\n",
    "        eta0=1e-3,\n",
    "        max_iter=300,\n",
    "    )\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "mean_cert = []\n",
    "\n",
    "for n_ids in range(100, 500, 100):\n",
    "    myids = small_ci_ids[0:n_ids]\n",
    "\n",
    "    y_true = y_test\n",
    "    y_pred = clf.predict(X_test[myids, :])\n",
    "\n",
    "    acc_sc = accuracy_score(y_true[myids], y_pred)\n",
    "    mean_cert.append(acc_sc)\n",
    "\n",
    "mean_cert = np.array(mean_cert)\n",
    "\n",
    "mean_uncert = []\n",
    "\n",
    "for n_ids in range(100, 500, 100):\n",
    "    myids = large_ci_ids[-n_ids:]\n",
    "\n",
    "    y_true = y_test\n",
    "    y_pred = clf.predict(X_test[myids, :])\n",
    "\n",
    "    acc_sc = accuracy_score(y_true[myids], y_pred)\n",
    "    mean_uncert.append(acc_sc)\n",
    "\n",
    "mean_uncert = np.array(mean_uncert)\n",
    "\n",
    "\n",
    "mean_mpi = np.mean(mean_cert - mean_uncert)\n",
    "std_mpi = np.std(mean_cert - mean_uncert)\n",
    "\n",
    "print(f\"MPI = {mean_mpi} +- {std_mpi}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3.7",
   "language": "python",
   "name": "venv3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
