{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db9c7a3d",
   "metadata": {
    "_uuid": "a171a930e23459ecbdb4d50dbffd2c18ca8e8ced"
   },
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1ba39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(\"..\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dad3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from src.data.data_loader import load_adult_data\n",
    "from src.models.benchmarks import comparison_methods\n",
    "from src.models.conformal import conformal_class\n",
    "from src.models.representation import compute_representation\n",
    "from src.utils.data_utils import (\n",
    "    covariance_comparison,\n",
    "    get_suspect_features,\n",
    "    read_from_file,\n",
    "    write_to_file,\n",
    ")\n",
    "from src.utils.helpers import inlier_outlier_dicts, sort_ci_vals\n",
    "\n",
    "model_ids = {}\n",
    "artifact_path = \"artifacts\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5993189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "data_augment = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464b9c19",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f97155a",
   "metadata": {
    "_uuid": "fbbeca12041ad15f3af35a5554028ee2dfa4c105",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, X, y = load_adult_data(split_size=0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2249ab",
   "metadata": {},
   "source": [
    "## Fit baseline downstream model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d4ece8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faa6506",
   "metadata": {},
   "source": [
    "## Step 1: Copula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e0dfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Running copula step...\")\n",
    "if data_augment:\n",
    "\n",
    "    # Fit copula & sample\n",
    "    # copula_samples = fit_sample_copula(clean_corpus=X_train, copula='vine', copula_n_samples=10000, columns=list(df.drop(['salary'],axis=1).columns), random_seed=100)\n",
    "    # write_to_file(copula_samples, f\"{artifact_path}/adult_copula_samples_10k.p\")\n",
    "\n",
    "    # To make things faster, we can read the samples from a file\n",
    "    copula_samples = read_from_file(f\"{artifact_path}/adult_copula_samples_10k.p\")\n",
    "\n",
    "else:\n",
    "    copula_samples = X_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852fdb65",
   "metadata": {},
   "source": [
    "### Get potentially suspicious features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85eae5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_suspects = covariance_comparison(clean_array=X_train, noisy_array=X_test)\n",
    "ks_suspect = get_suspect_features(clean_corpus=X_train, test_dataset=X_test, alpha=0.1)\n",
    "suspect_features = np.unique(np.append(cov_suspects, ks_suspect))\n",
    "suspect_features = np.unique(np.append(suspect_features, [0]))\n",
    "suspect_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c69f7f2",
   "metadata": {},
   "source": [
    "## 2. Representer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c091be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Running representer...\")\n",
    "\n",
    "pcs_train, pcs_test, pcs_copula = compute_representation(\n",
    "    train=X_train, test=X_test, copula_samples=X_train, n_components=8, rep_type=\"pca\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e797c175",
   "metadata": {},
   "source": [
    "## 3. Conformal Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c9a93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Running conformal predictor...\")\n",
    "\n",
    "means, stds = [], []\n",
    "\n",
    "bases = [\"tree\", \"mlp\", \"rf\", \"svm\", \"knn\"]\n",
    "\n",
    "\n",
    "base = bases[0]\n",
    "\n",
    "conformal_dict = {}\n",
    "for feat in suspect_features:\n",
    "    feat = int(feat)\n",
    "    dim = pcs_copula.shape[1]\n",
    "    conf = conformal_class(conformity_score=\"abs\", input_dim=dim, base_name=base)\n",
    "    conf.fit(x_train=pcs_copula, y_train=copula_samples[:, feat])\n",
    "    conformal_dict[feat] = conf.predict(x_test=pcs_test, y_test=X_test[:, feat])\n",
    "    logging.info(f\"Running analysis for feature = {feat}\")\n",
    "\n",
    "\n",
    "inliers_dict, outliers_dict = inlier_outlier_dicts(conformal_dict, suspect_features)\n",
    "\n",
    "\n",
    "small_ci_ids, large_ci_ids, df_out = sort_ci_vals(\n",
    "    conformal_dict, inliers_dict, suspect_features, proportion=0.5\n",
    ")\n",
    "model_ids[\"DS_small\"] = small_ci_ids\n",
    "model_ids[\"DS_large\"] = large_ci_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324cec93",
   "metadata": {},
   "source": [
    "## Compute MPIs & store for later analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b81e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_mean = {}\n",
    "benchmark_std = {}\n",
    "\n",
    "\n",
    "mean_cert = []\n",
    "mean_uncert = []\n",
    "\n",
    "for n_ids in range(100, 500, 100):\n",
    "    y_true = y_test\n",
    "\n",
    "    cert_ids = small_ci_ids[0:n_ids]\n",
    "    y_pred = clf.predict(X_test[cert_ids, :])\n",
    "    acc_sc = accuracy_score(y_true[cert_ids], y_pred)\n",
    "    mean_cert.append(acc_sc)\n",
    "\n",
    "    uncert_ids = large_ci_ids[-n_ids:]\n",
    "    y_pred = clf.predict(X_test[uncert_ids, :])\n",
    "    acc_sc = accuracy_score(y_true[uncert_ids], y_pred)\n",
    "    mean_uncert.append(acc_sc)\n",
    "\n",
    "mean_cert = np.array(mean_cert)\n",
    "mean_uncert = np.array(mean_uncert)\n",
    "\n",
    "\n",
    "benchmark_mean[\"DS\"] = np.mean(mean_cert - mean_uncert)\n",
    "benchmark_std[\"DS\"] = np.std(mean_cert - mean_uncert)\n",
    "\n",
    "val = benchmark_mean[\"DS\"]\n",
    "means.append(val)\n",
    "\n",
    "val = benchmark_std[\"DS\"]\n",
    "stds.append(val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edea990",
   "metadata": {},
   "source": [
    "# Run comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aaf7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_models = [\"qr\", \"bnn\", \"conformal\", \"mcd\", \"ensemble\", \"gp\"]\n",
    "\n",
    "for model in comparison_models:\n",
    "    logging.info(f\"Evaluating benchmark: {model}\")\n",
    "    uncertainty_scores = []\n",
    "\n",
    "    for feat in range(X_train.shape[1]):\n",
    "        indices = list(range(X_train.shape[1]))\n",
    "        indices.remove(feat)\n",
    "\n",
    "        ids = range(X_test.shape[0])\n",
    "        uncertainty_score = comparison_methods(\n",
    "            x_train=X_train[:, indices],\n",
    "            y_train=X_train[:, feat],\n",
    "            x_test=X_test[:, indices],\n",
    "            y_test=X_test[:, feat],\n",
    "            inlier_ids=ids,\n",
    "            df_inlier=None,\n",
    "            model_type=model,\n",
    "            return_ids=False,\n",
    "        )\n",
    "\n",
    "        uncertainty_scores.append(uncertainty_score)\n",
    "\n",
    "    ordered_scores = np.argsort([sum(i) for i in zip(*uncertainty_scores)])\n",
    "    model_ids[model] = ordered_scores\n",
    "\n",
    "    model_certainty = []\n",
    "    model_uncertainty = []\n",
    "\n",
    "    samples = np.arange(100, 500, 100)\n",
    "    for sample in samples:\n",
    "        certain = ordered_scores[0:sample]\n",
    "        uncertain = ordered_scores[-sample:]\n",
    "\n",
    "        y_pred = clf.predict(X_test[certain, :])\n",
    "        model_certainty.append(accuracy_score(y_test[certain], y_pred))\n",
    "\n",
    "        y_pred = clf.predict((X_test[uncertain, :]))\n",
    "        model_uncertainty.append(accuracy_score(y_test[uncertain], y_pred))\n",
    "\n",
    "    diff_mean = np.mean(np.array(model_certainty) - np.array(model_uncertainty))\n",
    "    diff_std = np.std(np.array(model_certainty) - np.array(model_uncertainty))\n",
    "\n",
    "    benchmark_mean[model] = diff_mean\n",
    "    benchmark_std[model] = diff_std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cadd99e",
   "metadata": {},
   "source": [
    "## Capture artifacts for later analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581d91eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts = {}\n",
    "\n",
    "artifacts[\"clf\"] = clf\n",
    "artifacts[\"conformal_dict\"] = conformal_dict\n",
    "artifacts[\"inliers_dict\"] = inliers_dict\n",
    "artifacts[\"outliers_dict\"] = outliers_dict\n",
    "artifacts[\"benchmark_mean\"] = benchmark_mean\n",
    "artifacts[\"benchmark_std\"] = benchmark_std\n",
    "artifacts[\"small_ci_ids\"] = small_ci_ids\n",
    "artifacts[\"large_ci_ids\"] = large_ci_ids\n",
    "artifacts[\"df_out\"] = df_out\n",
    "artifacts[\"X_train\"] = X_train\n",
    "artifacts[\"X_test\"] = X_test\n",
    "artifacts[\"y_train\"] = y_train\n",
    "artifacts[\"y_test\"] = y_test\n",
    "artifacts[\"X\"] = X\n",
    "artifacts[\"y\"] = y\n",
    "\n",
    "write_to_file(artifacts, f\"{artifact_path}/adult_dataset_artifacts.p\")\n",
    "write_to_file(model_ids, f\"{artifact_path}/adult_model_ids.p\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4f6c89",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f731ff54",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47e1ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_std\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3.7",
   "language": "python",
   "name": "venv3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
