{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68c38648",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e59747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(\"..\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "model_ids = {}\n",
    "artifact_path = \"artifacts\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28eda79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from src.data.data_loader import load_electric\n",
    "from src.models.benchmarks import comparison_methods\n",
    "from src.models.conformal import conformal_class\n",
    "from src.models.copula import fit_sample_copula\n",
    "from src.models.representation import compute_representation\n",
    "from src.utils.data_utils import (\n",
    "    covariance_comparison,\n",
    "    get_suspect_features,\n",
    "    write_to_file,\n",
    ")\n",
    "from src.utils.helpers import inlier_outlier_dicts, sort_ci_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ee85e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "data_augment = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4786c335",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bc4a33",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "logging.info(\"Loading data...\")\n",
    "(\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    ") = load_electric()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f064ce",
   "metadata": {},
   "source": [
    "## Fit baseline downstream model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fa56ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Training downstream model...\")\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2835c2c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "cov_suspects = covariance_comparison(\n",
    "    clean_array=X_train.to_numpy(), noisy_array=X_test.to_numpy()\n",
    ")\n",
    "ks_suspect = get_suspect_features(\n",
    "    clean_corpus=X_train.to_numpy(), test_dataset=X_test.to_numpy(), alpha=0.1\n",
    ")\n",
    "suspect_features = np.unique(np.append(cov_suspects, ks_suspect))\n",
    "suspect_features = np.unique(np.append(suspect_features, [0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee1c226",
   "metadata": {},
   "source": [
    "## Step 1: Copula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeec8325",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "logging.info(\"Running copula step...\")\n",
    "\n",
    "if data_augment:\n",
    "\n",
    "    copula_samples = fit_sample_copula(\n",
    "        clean_corpus=X_train,\n",
    "        copula=\"vine\",\n",
    "        copula_n_samples=10000,\n",
    "        columns=list(X_train.columns),\n",
    "        random_seed=64,\n",
    "    )\n",
    "\n",
    "else:\n",
    "    copula_samples = X_train.to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce2fff6",
   "metadata": {},
   "source": [
    "## 2. Representer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34da8ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Running representer...\")\n",
    "pcs_train, pcs_test, pcs_copula = compute_representation(\n",
    "    train=X_train, test=X_test, copula_samples=X_train, n_components=4, rep_type=\"pca\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208cbad3",
   "metadata": {},
   "source": [
    "## 3. Conformal Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397fc8d0",
   "metadata": {
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [],
   "source": [
    "logging.info(\"Running conformal predictor...\")\n",
    "means, stds = [], []\n",
    "\n",
    "\n",
    "bases = [\"tree\", \"rf\", \"svm\", \"knn\"]\n",
    "\n",
    "base = bases[0]\n",
    "\n",
    "\n",
    "conformal_dict = {}\n",
    "for feat in suspect_features:\n",
    "    feat = int(feat)\n",
    "    dim = pcs_copula.shape[1]\n",
    "    conf = conformal_class(conformity_score=\"abs\", input_dim=dim, base_name=base)\n",
    "    conf.fit(x_train=pcs_copula, y_train=copula_samples[:, feat])\n",
    "    conformal_dict[feat] = conf.predict(\n",
    "        x_test=pcs_test, y_test=X_test.to_numpy()[:, feat]\n",
    "    )\n",
    "    logging.info(f\"Running analysis for feature = {feat}\")\n",
    "\n",
    "inliers_dict, outliers_dict = inlier_outlier_dicts(conformal_dict, suspect_features)\n",
    "\n",
    "\n",
    "small_ci_ids, large_ci_ids, df_out = sort_ci_vals(\n",
    "    conformal_dict, inliers_dict, suspect_features, proportion=0.5\n",
    ")\n",
    "model_ids[\"DS_small\"] = small_ci_ids\n",
    "model_ids[\"DS_large\"] = large_ci_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6ed94b",
   "metadata": {},
   "source": [
    "## Compute MPIs & store for later analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc77c231",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_mean = {}\n",
    "benchmark_std = {}\n",
    "\n",
    "mean_cert = []\n",
    "mean_uncert = []\n",
    "\n",
    "for n_ids in range(100, 1000, 100):\n",
    "    y_true = y_test\n",
    "\n",
    "    cert_ids = small_ci_ids[0:n_ids]\n",
    "    y_pred = clf.predict(X_test.to_numpy()[cert_ids, :])\n",
    "    acc_sc = accuracy_score(y_true.to_numpy()[cert_ids], y_pred)\n",
    "    mean_cert.append(acc_sc)\n",
    "\n",
    "    uncert_ids = large_ci_ids[-n_ids:]\n",
    "    y_pred = clf.predict(X_test.to_numpy()[uncert_ids, :])\n",
    "    acc_sc = accuracy_score(y_true.to_numpy()[uncert_ids], y_pred)\n",
    "    mean_uncert.append(acc_sc)\n",
    "\n",
    "mean_cert = np.array(mean_cert)\n",
    "mean_uncert = np.array(mean_uncert)\n",
    "\n",
    "\n",
    "benchmark_mean[\"DS\"] = np.mean(mean_cert - mean_uncert)\n",
    "benchmark_std[\"DS\"] = np.std(mean_cert - mean_uncert)\n",
    "\n",
    "val = benchmark_mean[\"DS\"]\n",
    "means.append(val)\n",
    "\n",
    "val = benchmark_std[\"DS\"]\n",
    "stds.append(val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89be4bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2015cff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5622e03f",
   "metadata": {},
   "source": [
    "# Run comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539dfbee",
   "metadata": {
    "lines_to_end_of_cell_marker": 2,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "comparison_models = [\"qr\", \"bnn\", \"conformal\", \"mcd\", \"ensemble\", \"gp\"]\n",
    "\n",
    "for model in comparison_models:\n",
    "    logging.info(f\"Evaluating benchmark: {model}\")\n",
    "    uncertainty_scores = []\n",
    "\n",
    "    for feat in range(X_train.shape[1]):\n",
    "        indices = list(range(X_train.shape[1]))\n",
    "        indices.remove(feat)\n",
    "\n",
    "        print(feat)\n",
    "\n",
    "        ids = range(X_test.shape[0])\n",
    "        uncertainty_score = comparison_methods(\n",
    "            x_train=X_train.to_numpy()[:, indices],\n",
    "            y_train=X_train.to_numpy()[:, feat],\n",
    "            x_test=X_test.to_numpy()[:, indices],\n",
    "            y_test=X_test.to_numpy()[:, feat],\n",
    "            inlier_ids=ids,\n",
    "            df_inlier=None,\n",
    "            model_type=model,\n",
    "            return_ids=False,\n",
    "        )\n",
    "\n",
    "        uncertainty_scores.append(uncertainty_score)\n",
    "\n",
    "    ordered_scores = np.argsort([sum(i) for i in zip(*uncertainty_scores)])\n",
    "    model_ids[model] = ordered_scores\n",
    "\n",
    "    model_certainty = []\n",
    "    model_uncertainty = []\n",
    "\n",
    "    samples = np.arange(100, 5000, 100)\n",
    "    for sample in samples:\n",
    "        certain = ordered_scores[0:sample]\n",
    "        uncertain = ordered_scores[-sample:]\n",
    "\n",
    "        y_pred = clf.predict(X_test.to_numpy()[certain, :])\n",
    "        model_certainty.append(accuracy_score(y_test.to_numpy()[certain], y_pred))\n",
    "\n",
    "        y_pred = clf.predict((X_test.to_numpy()[uncertain, :]))\n",
    "        model_uncertainty.append(accuracy_score(y_test.to_numpy()[uncertain], y_pred))\n",
    "\n",
    "    diff_mean = np.mean(np.array(model_certainty) - np.array(model_uncertainty))\n",
    "    diff_std = np.std(np.array(model_certainty) - np.array(model_uncertainty))\n",
    "\n",
    "    benchmark_mean[model] = diff_mean\n",
    "    benchmark_std[model] = diff_std\n",
    "\n",
    "    print(model, \" : \", diff_mean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcbb333",
   "metadata": {},
   "source": [
    "## Capture artifacts for later analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51387f40",
   "metadata": {
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [],
   "source": [
    "artifacts = {}\n",
    "\n",
    "artifacts[\"clf\"] = clf\n",
    "artifacts[\"conformal_dict\"] = conformal_dict\n",
    "artifacts[\"inliers_dict\"] = inliers_dict\n",
    "artifacts[\"outliers_dict\"] = outliers_dict\n",
    "artifacts[\"benchmark_mean\"] = benchmark_mean\n",
    "artifacts[\"benchmark_std\"] = benchmark_std\n",
    "artifacts[\"small_ci_ids\"] = small_ci_ids\n",
    "artifacts[\"large_ci_ids\"] = large_ci_ids\n",
    "artifacts[\"df_out\"] = df_out\n",
    "artifacts[\"X_train\"] = X_train\n",
    "artifacts[\"X_test\"] = X_test\n",
    "artifacts[\"y_train\"] = y_train\n",
    "artifacts[\"y_test\"] = y_test\n",
    "\n",
    "\n",
    "write_to_file(artifacts, f\"{artifact_path}/electric_dataset_artifacts.p\")\n",
    "write_to_file(model_ids, f\"{artifact_path}/electric_model_ids.p\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7bfc8f",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6639a1ff",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "benchmark_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f03b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_std\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3.7",
   "language": "python",
   "name": "venv3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
